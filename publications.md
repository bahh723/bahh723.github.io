---
title: "Publications"
permalink: "/publications/"
layout: page
---

## Preprints <span style="font-size: 17px;">(* indicates equal contribution or alphabetical ordering)</span> 
 - **Policy Optimization in Adversarial MDPs:  Improved Exploration via Dilated Bonuses**  
Haipeng Luo\*, Chen-Yu Wei\*, Chung-Wei Lee  
[[arXiv](https://arxiv.org/abs/2107.08346){:target="_blank"}]

## Conference Papers

### 2021

 - **Achieving Near Instance-Optimality and Minimax-Optimality in Stochastic and Adversarial Linear Bandits Simultaneously**  
Chung-Wei Lee\*, Haipeng Luo\*, Chen-Yu Wei\*, Mengxiao Zhang\*, Xiaojin Zhang\*  
*ICML 2021*  
[[arXiv](https://arxiv.org/abs/2102.05858){:target="_blank"}]

 - **Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach**  
Chen-Yu Wei and Haipeng Luo  
*COLT 2021*  <span style="color:red">(Best Paper Award)</span>  
[[arXiv](https://arxiv.org/abs/2102.05406){:target="_blank"}]

- **Last-iterate Convergence of Decentralized Optimistic Gradient Descent/Ascent in Infinite-horizon Competitive Markov Games**  
Chen-Yu Wei, Chung-Wei Lee\*, Mengxiao Zhang\*, Haipeng Luo  
*COLT 2021*  
[[arXiv](https://arxiv.org/abs/2102.04540){:target="_blank"}]

- **Impossible Tuning Made Possible: A New Expert Algorithm and Its Applications**  
Liyu Chen\*, Haipeng Luo\*, Chen-Yu Wei\*  
*COLT 2021*  
[[arXiv](https://arxiv.org/abs/2102.01046){:target="_blank"}]

- **Minimax Regret for Stochastic Shortest Path with Adversarial Costs and Known Transition**  
Liyu Chen, Haipeng Luo, Chen-Yu Wei  
*COLT 2021*  
[[arXiv](https://arxiv.org/abs/2012.04053){:target="_blank"}]

- **Learning Infinite-horizon Average-reward MDPs with Linear Function Approximation**  
Chen-Yu Wei, Mehdi Jafarnia-Jahromi, Haipeng Luo, Rahul Jain  
*AISTAT 2021*  
[[arXiv](https://arxiv.org/abs/2007.11849){:target="_blank"}]

- **Linear Last-iterate Convergence in Constrained Saddle-point Optimization**  
Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, Haipeng Luo  
*ICLR 2021*  
[[arXiv](https://arxiv.org/abs/2006.09517){:target="_blank"}] [[code](https://github.com/bahh723/OGDA-last-iterate){:target="_blank"}]
[[slides](https://drive.google.com/file/d/1Zr9BH7nX4BknXoALIcJb-gsVHNEeUEH0/view?usp=sharing){:target="_blank"}]

- **Adversarial Online Learning with Changing Action Sets: Efficient Algorithms with Approximate Regret Bounds**  
Ehsan Emamjomeh-Zadeh\*, Chen-Yu Wei\*, Haipeng Luo, David Kempe  
*ALT 2021*  
[[arXiv](https://arxiv.org/abs/2003.03490){:target="_blank"}]

### 2020

- **Bias no more: high-probability data-dependent regret bounds for adversarial bandits and MDPs**  
Chung-Wei Lee\*, Haipeng Luo\*, Chen-Yu Wei\*, Mengxiao Zhang\*  
*NeurIPS 2020*  <span style="color:red">(Oral)</span>  
[[arXiv](https://arxiv.org/abs/2006.08040){:target="_blank"}]

- **Federated Residual Learning**  
Chen-Yu Wei, Alekh Agarwal, John Langford  
*NeurIPS Workshop on Scalability, Privacy, and Security in Federated Learning 2020*  
[[arXiv](https://arxiv.org/abs/2003.12880){:target="_blank"}]

- **Taking a Hint: How to Leverage Loss Predictors in Contextual Bandits?**  
Chen-Yu Wei, Haipeng Luo, Alekh Agarwal  
*COLT 2020*  
[[arXiv](https://arxiv.org/abs/2003.01922){:target="_blank"}]

- **Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes**  
Chen-Yu Wei, Mehdi Jafarnia-Jahromi, Haipeng Luo, Hiteshi Sharma, Rahul Jain  
*ICML 2020*  
[[arXiv](https://arxiv.org/abs/1910.07072){:target="_blank"}]
[[code](https://github.com/bahh723/model-free-rl-algos){:target="_blank"}]

### 2019

- **Analyzing the Variance of Policy Gradient Estimators for the Linear-Quadratic Regulator**  
James Preiss\*, Sebastien Arnold\*, Chen-Yu Wei\*, Marius Kloft  
*NeurIPS Workshop on Optimization Foundations for Reinforcement Learning 2019*  
*SoCal Machine Learning Symposium 2019* <span style="color:red">(Best Poster Award)</span>  
[[arXiv](https://arxiv.org/abs/1910.01249){:target="_blank"}]

- **A New Algorithm for Non-stationary Contextual Bandits: Efficient, Optimal, and Parameter-free**  
Yifang Chen\*, Chung-Wei Lee\*, Haipeng Luo\*, Chen-Yu Wei\*  
*COLT 2019*  
[[arXiv](https://arxiv.org/abs/1902.00980){:target="_blank"}]
[[A joint extended abstract with Auer, Gajane, and Ortner](http://proceedings.mlr.press/v99/auer19b.html){:target="_blank"}]

- **Improved Path-length Regret Bounds for Bandits**  
Sebastien Bubeck\*, Yuanzhi Li\*, Haipeng Luo\*, Chen-Yu Wei\*  
*COLT 2019*  
[[arXiv](https://arxiv.org/abs/1901.10604){:target="_blank"}]

- **Bandit Multiclass Linear Classification: Efficient Algorithms for the Separable Case**  
Alina Beygelzimer\*, David Pal\*, Balazs Szorenyi\*, Devanathan Thiruvenkatachari\*, Chen-Yu Wei\*, Chicheng Zhang\*  
*ICML 2019*  
[[arXiv](https://arxiv.org/abs/1902.02244){:target="_blank"}]
[[code](https://github.com/bahh723/separable-bandit-classification){:target="_blank"}]

- **Beating Stochastic and Adversarial Semi-bandits Optimally and Simultaneously**  
Julian Zimmert, Haipeng Luo, Chen-Yu Wei  
*ICML 2019* <span style="color:red">(Long talk)</span>  
[[arXiv](https://arxiv.org/abs/1901.08779){:target="_blank"}]

### 2018

- **Efficient Online Portfolio with Logarithmic Regret**  
Haipeng Luo\*, Chen-Yu Wei\*, Kai Zheng\*  
*NeurIPS 2018* <span style="color:red">(Spotlight)</span>  
[[arXiv](https://arxiv.org/abs/1708.01799){:target="_blank"}]

- **Efficient Contextual Bandits in Non-stationary Worlds**  
Haipeng Luo\*, Chen-Yu Wei\*, Alekh Agarwal, John Langford  
*COLT 2018*  
[[arXiv](https://arxiv.org/abs/1708.01799){:target="_blank"}]

- **More Adaptive Algorithms for Adversarial Bandits**  
Chen-Yu Wei and Haipeng Luo  
*COLT 2018*  
[[arXiv](https://arxiv.org/abs/1801.03265){:target="_blank"}]

### 2017

- **Online Reinforcement Learning in Stochastic Games**  
Chen-Yu Wei, Yi-Te Hong, Chi-Jen Lu  
*NeurIPS 2017*  
[[arXiv](https://arxiv.org/abs/1712.00579){:target="_blank"}]

### 2016

- **Tracking the Best Expert in Non-stationary Stochastic Environments**  
Chen-Yu Wei, Yi-Te Hong, Chi-Jen Lu  
*NeurIPS 2016*  
[[arXiv](https://arxiv.org/abs/1712.00578){:target="_blank"}]
