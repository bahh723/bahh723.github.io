---
title: "Reinforcement Learning (Spring 2026)"
permalink: "/rl2026sp/"
layout: page-course
show_navigation: false
---

<br/><br>   

## Course Information
- **Time**: Tuesday & Thursday 2:00-3:15PM  
- **Location**: Olsson Hall 005       
- **Instructor and office hours**: [Chen-Yu Wei](https://bahh723.github.io/), Monday 3:30-4:30PM @ Rice 409       
- **TA and office hours**:  
    * [Fengyu Gao](https://gfengyu.github.io/){:target="_blank"},  Tuesday 3:30-4:30PM @ Rice 328      
    * [Xinyu Liu](https://saodimao20.github.io/){:target="_blank"}, Wednesday 4:00-5:00PM @ Rice 442     
    * Yufeng Gao, Thursday 3:30-4:30PM @ Rice 442       
    



## Overview  
Reinforcement learning (RL) is a powerful learning paradigm through which machines learn to make (sequential) decisions. It has been playing a pivotal role in advancing artificial intelligence, with notable successes including mastering the game of Go and enhancing large language models.  

This course focuses on the design principles of RL algorithms. Similar to statistical learning, a central challenge in RL is to **generalize** learned capabilities to unseen environments.  However, RL also faces additional challenges such as **exploration-exploitation tradeoff**, **credit assignment**, and **distribution mismatch** between behavior and target policies. Throughout the course, we will delve into various solutions to these challenges.  

## Prerequisites  
Probability, linear algebra, calculus, machine learning, python programming    

## Platforms
- [Piazza](https://piazza.com/class/mkcthnb84r74gm/){:target="_blank"}: Discussions   
- [Gradescope](https://www.gradescope.com/courses/1238160){:target="_blank"}: Homework submissions

## Grading
- (70%) **Assignments**: 5-6 assignments           
- (30%) **Exams**:  midterm (12%) and final (18%)   

**Assignments policy**: Students have 12 free late days that may be used across all assignments. After the free late days are exhausted, each additional late day incurs a 10% deduction from the semester assignment grade. **No assignment can be submitted more than 7 days after its deadline.** Late days are counted by rounding up (e.g., 1 hour late counts as 1 day late). 

**Exams policy**: **All exams are in person**; no online option is available. For both the midterm and the final, one (and at most one) make-up exam session may be arranged. If you miss the midterm due to extenuating circumstances (e.g., illness, family emergency), the final exam may be used to replace the midterm score. If you miss the final exam due to extenuating circumstances, you may request an [incomplete grade](https://college.as.virginia.edu/incomplete-grade-extension-time-form-and-instructions){:target="_blank"} and complete the exam after the semester. 

**Bonus points**:  Completing the course evaluation at semester end earns 3 bonus points. 

The mapping from scores to final grades does NOT always follow the [default scale](https://virginia.service-now.com/its?id=itsweb_kb_article&sys_id=1153c16fdba41f444f32fb671d961934){:target="_blank"}. It may be adjusted (towards a better grade) based on the score distribution.    



## Schedule    

One file of slides may be used for multiple lectures. Check Piazza for the recording passcode.   

{: .week-schedule} 
| Date    | Topics    |  Slides  | Recordings |  Assignments  |
|:----------------|:----------------|:----------------|:----------------|:----------------|
| 1/13 | **Introduction** | [Slides](/rl2026sp_files/introduction.pdf){:target="_blank"} | No recording |  |
| 1/15 |  **Value-based bandit algorithms**: Explore-then-commit, &epsilon;-greedy | [Slides](/rl2026sp_files/bandits1.pdf){:target="_blank"} | [Recording](https://virginia.zoom.us/rec/share/buROPpVnflIvHmoRFmo9-SCJvBl4pvIkyP1lkrSxjgEJITjp4p2wVr6RflPzyx4i.zKbPEK_wGBv46PRo){:target="_blank"} |  |
| 1/20 | Boltzmann exploration |  | [Recording](https://virginia.zoom.us/rec/share/8jxXWGl_XYnRSC7cWT_bJ5BOzk2vXzpQbJBERITe5ZDhBtf1ZpbIW7uq6xjI3eaY.R6Esc7NBt3sHT73H){:target="_blank"} |  |
| 1/22 | Contextual bandits with regression |  | [Recording](https://virginia.zoom.us/rec/share/oEOzvmOfHol07MeJJZVzWZYlnJcmz7gR67TZedTSkH9H93YzlC6CkLzmDVWWz3cj.6JVOYE8SzMSUIMee){:target="_blank"} |  [HW1](/rl2026sp_files/HW1.pdf){:target="_blank"} out |
| 1/27 | UCB, TS |  | [Recording](https://virginia.zoom.us/rec/share/mifc2taoXJPY4hLIzIrlNsXrZvoJI9BesKKLgT-th3CFT5UGizYo5uzN_1O3gzu5.925eff0JyzLbmvja){:target="_blank"} |  |
| 1/29 | **Policy-based bandit algorithms** | [Slides](/rl2026sp_files/bandits2.pdf){:target="_blank"} | [Recording](https://virginia.zoom.us/rec/share/iXCqsLpDkOuIHcIs3EK0gW2bEGnEULt01L7tEqWkyO95Q0JzigcUaxxSQa8PByA4.k32_CKp2JBRYurX5){:target="_blank"} |  |
| 2/3 |  |  |  | HW1 due on 2/4 | 
| 2/5 |  |  |  |  |
| 2/10 |  |  |  |  |
| 2/12 | **Markov decision processes** |  |  |  |
| 2/17 |  |  |  |  |
| 2/19 |  |  |  |  |
| 2/24 |  |  |  |  |
| 2/26 | **Midterm Exam** (in class) |  |  |  |
| 3/3 | <span style="color:#aaaaaa">Spring recess (no class)</span> |  |  |  |
| 3/5 | <span style="color:#aaaaaa">Spring recess (no class)</span> |  |  |  |
| 3/10 |  |  |  |  |
| 3/12 |  |  |  |  |
| 3/17 | **Value-based RL algorithms** |  |  |  |
| 3/19 |  |  |  |  |
| 3/24 |  |  |  |  |
| 3/26 | **Policy-based RL algorithms** |  |  |  |
| 3/31 |  |  |  |  |
| 4/2 |  |  |  |  |
| 4/7 | **RL with models/simulators** |  |  |  |
| 4/9 |  |  |  |  |
| 4/14 |  |  |  |  |
| 4/16 | **Imitation learning** |  |  |  |
| 4/21 |  |  |  |  |
| 4/23 |  |  |  |  |
| 4/28 | <span style="color:#aaaaaa">Last Lecture</span> |  |  |  |
| 5/1 | **Final Exam** (9AM-12PM) |  |  |  |






## Resources
- [Deep Reinforcement Learning](https://rail.eecs.berkeley.edu/deeprlcourse/){:target="_blank"} by Sergey Levine     
- [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html){:target="_blank"} by Richard Sutton and Andrew Barto   
- [Reinforcement Learning: A Comprehensive Overview](https://arxiv.org/pdf/2412.05265){:target="_blank"} by Kevin Murphy   
- [Bandit Algorithms](https://tor-lattimore.com/downloads/book/book.pdf){:target="_blank"} by Tor Lattimore and Csaba Szepesvari   






