---
title: "Reinforcement Learning (Spring 2025)"
permalink: "/rl2025sp/"
layout: page-course
show_navigation: false
---

<br/><br>

## Course Information
- **Time**: Monday & Wednesday 9:30-10:45AM  
- **Location**: Rice 340   
- **Instructor and office hours**: [Chen-Yu Wei](https://bahh723.github.io/), Tuesday 3-4PM @ Rice 409     
- **TA and office hours**: [Braham Snyder](https://www.braham.io/){:target="_blank"}, Friday 4-5PM @ Rice 442   


## Overview  
Reinforcement learning (RL) is a powerful learning paradigm through which machines learn to make (sequential) decisions. It has been playing a pivotal role in advancing artificial intelligence, with notable successes including <a href="https://www.nature.com/articles/nature16961" target="_blank">mastering the game of Go</a> and <a href="https://openai.com/index/instruction-following/" target="_blank">enhancing large language models</a>.  

This course focuses on the design principles of RL algorithms. Similar to statistical learning, a central challenge in RL is to **generalize** learned capabilities to unseen environments.  However, RL also faces additional challenges such as **exploration-exploitation tradeoff**, **credit assignment**, and **distribution mismatch** between behavior and target policies. Throughout the course, we will delve into various solutions to these challenges and provide theoretical justifications.  

## Prerequisites  
This course is mathematically demanding. Students are expected to have strong foundations in **probability**, **linear algebra**, and **calculus**. A basic understanding of machine learning and convex optimization will be beneficial. Proficiency in **python** programming is required. 

## Topics 
Bandits, online learning, dynamic programming, Q-learning, policy evaluation, policy gradient. 

## Platforms
- [Piazza](https://piazza.com/class/m5v3ed2f1f63ei/){:target="_blank"}: Discussions  
- [Gradescope](https://www.gradescope.com/courses/968387){:target="_blank"}: Homework submissions  


## Grading
- (70%) **Assignments**        
- (30%) **Final project** ([spec](/rl2025sp_files/final-project.pdf){:target="_blank"})        

**Late policy for assignments**: 10 free late days can be used across all assignments. Each additional late day will result in a 10% deduction in the semester's assignment grade.  No assignment can be submitted more than 7 days after its deadline.  



## Schedule

One slide deck may be used for multiple lectures. 

| Date    | Topics    |  Materials   |  Assignments  |
|:----------------|:----------------|:----------------|:----------------|
| 1/13 | **Introduction** | [Slides](/rl2025sp_files/introduction.pdf){:target="_blank"}, [Recording](https://virginia.zoom.us/rec/share/LOizdQbexpbBT8RsRuRujEIjsJPN2C7Vkxrnt7rv_w-7nBf9sPFuWj-4sTWGr-qD.Usoe7v8ynvlY6Ddi){:target="_blank"} | [HW0](/rl2025sp_files/HW0.pdf){:target="_blank"} (no submission needed) |
| 1/15 | **Value-based bandits**: Explore-then-exploit, &epsilon;-greedy | [Slides](/rl2025sp_files/bandits1.pdf){:target="_blank"}, [Recording](https://virginia.zoom.us/rec/share/6r_YblVPYKHMTHvusfD9ybQJuCtAfX2tcCqkVqBJ5GEyN9DNZEcrnVgI_wU-QYWY.WL2zs2sFlOWpUf_e){:target="_blank"} |  |
| 1/20 | <span style="color:lightgray">MLK Holiday</span>  |  |  | 
| 1/22 | Boltzmann exploration, Inverse gap weighting, Reduction | [Recording](https://virginia.zoom.us/rec/share/mpNKMAsgr_RpvKJi5jE2fpHc2rjvx35LLGJNPQzMZboSOJlSmqz3hpd_TaL23k9T.0Ly0ut2A7fyfiJy6){:target="_blank"}, [Supp-IGW](/rl2025sp_files/igw.pdf){:target="_blank"} |  |
| 1/27 | UCB, TS | [Recording](https://virginia.zoom.us/rec/share/kItwByOYyq2i8dfIY4mzoTn5WD2vTfe1oTKHTdDhJLwTwrdzB1o43aJDKSntZ7rD.2z_XuQDLVSzO5h7G){:target="_blank"} | [HW1](/rl2025sp_files/HW1.pdf) out |
| 1/29 | **Policy-based bandits**: Exponential weights (full-information) | [Slides](/rl2025sp_files/bandits2.pdf){:target="_blank"}, [Recording](https://virginia.zoom.us/rec/share/5uLjWL23bqaYIFa1Q021cOVE93TsigiNH4HZjvIDrtTnOP4X-rQb3awH-bV5P1Mz.Z9D0IZ2eLmylsFgf){:target="_blank"} |  |
| 2/3 | EXP3 | [Recording](https://virginia.zoom.us/rec/share/6qOFhOvmILUKuwgY8aJ5yJwnHQgOuIGpck2-rjU0GhUm5uvwnmNlo0wj6SZOstok.AX0hnpi_MW1jq3iY){:target="_blank"} |  |
| 2/5 | PPO | [Recording](https://virginia.zoom.us/rec/share/FH6LEG6OIoivCApAEQdsdxHKz15aDbPa_D6s36qVLzm5KyZcUWMOklnfY9Qd0UNz.EQDGKgyDlteWx0Up){:target="_blank"} | HW1 due on 2/7 |
| 2/10 | NPG, PG | [Recording](https://virginia.zoom.us/rec/share/5LJDB6Sy0HzItbHYx8-71gUFSIC0Kj7lcu8cHua2PZUl9CpSVEc00EMUMpKus8JB.278SAGuQjr88hVVp){:target="_blank"} |  |
| 2/12 | **Bandits with continuous actions**: Gradient ascent | [Slides](/rl2025sp_files/continuous-bandits.pdf){:target="_blank"}, [Recording](https://virginia.zoom.us/rec/share/gW7AUsN1jfECZnKPCmydkZb5RpCby7a4w1tWIy3rKOBUFzi0eY9M4Y9EtmcZ_tnA.pahM6dCjLb95wJWr){:target="_blank"} | [HW2](/rl2025sp_files/HW2.pdf){:target="_blank"} out |
| 2/17 | One-point gradient estimators | [Recording](https://virginia.zoom.us/rec/share/35HVd3ly1t2LxyQyl2iUGFDKvIxUHKGmgHS_imPKSlaPyLastkYuw_FdypC5B6Xz.dQzf4t6Flhsu6jIW){:target="_blank"} |  |
| 2/19 | PG, PPO | [Recording](https://virginia.zoom.us/rec/share/6Cb6x7ZmzVw_PkpErXDZSrS3sBQPM1QfqL4hf8mtQxxNFEeeHR7pC-qcusmR-IJl.soQd8G4Ncufeg8b3){:target="_blank"} |  |
| 2/24 | **Markov decision process** | [Slides](/rl2025sp_files/mdp.pdf){:target="_blank"}, [Recording](https://virginia.zoom.us/rec/share/XVgcohDHkDRnAJVXGoVbANjaU3-gxZtFiYM9Y-VJXVkZp3banAZesg0vOh1KNg6W.DdxgGNzshh-yqJNr){:target="_blank"} |  |
| 2/26 | Dynamic programming| [Recording](https://virginia.zoom.us/rec/share/JCOwDal2Ug_f1f844NeRqh6WfEoFTIQBJcddDAXMzky3VOJQl1uBnQqWtpS9D7zH.hnV_48d2HGnK1eHB){:target="_blank"} | HW2 due on 2/28 |
| 3/3 | Dynamic programming | [Recording](https://virginia.zoom.us/rec/share/Dm9DZ99YPwhbWP9ZoZNf3Is7JXlkKgXVukIujWoDm1hsUV3ANYejpkC6dNtX_iC-.KS2t5AfM8coss6Uq){:target="_blank"} |  |
| 3/5 | Dynamic programming | [Recording](https://virginia.zoom.us/rec/share/42WRz0fzCyvqBCNlj-KUS_orczNmHyc6SnVOkxpwHD72XmLvyc4Ogv1OBtxfQjka.4atiHtk62lljberk){:target="_blank"} |  |
| 3/10 | <span style="color:lightgray">Spring recess</span>  |  |  |
| 3/12 | <span style="color:lightgray">Spring recess</span> |  |  |
| 3/17 | Performance difference lemma | [Recording](https://virginia.zoom.us/rec/share/qS89VuI_5ujo8sgZCtx5DsI_HGkSRc-uTNyne_pVL1xQ9cFM2mukON4IOhxDQSUP.8Ft_5PZkEmSvrCoV){:target="_blank"} | [HW3](/rl2025sp_files/HW3.pdf){:target="_blank"} out |
| 3/19 | Performance difference lemma | [Recording](https://virginia.zoom.us/rec/share/aaskF8yEvKsIqCOdJzGOSVfW0uZmVR2xF9LXo710GkzIDZNpG3JPFxBV0dAfBUzK.Tdr67PxTnYLLYHtJ){:target="_blank"} |  |
| 3/24 | **Value-based RL**: Deep Q-learning |  |  |
| 3/26 |  |  | Midterm report due on 3/28 <br/> HW3 due on 3/30 |
| 3/31 |  |  |  |
| 4/2 |  |  |  |
| 4/7 |  |  |  |
| 4/9 |  |  |  |
| 4/14 |  |  |  |
| 4/16 |  |  |  |
| 4/21 |  |  |  |
| 4/23 |  |  |  |
| 4/28 |  |  |  |

## Resources
- [Bandit Algorithms](https://tor-lattimore.com/downloads/book/book.pdf){:target="_blank"} by Tor Lattimore and Csaba Szepesvari   
- [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html){:target="_blank"} by Richard Sutton and Andrew Barto  
- [Reinforcement Learning: Theory and Algorithms](https://rltheorybook.github.io/){:target="_blank"} by Alekh Agarwal, Nan Jiang, Sham Kakade, and Wen Sun  
- [Statistical Reinforcement Learning and Decision Making: Course Notes](https://www.mit.edu/~rakhlin/courses/course_stat_rl/course_stat_rl.pdf){:target="_blank"} by Dylan Foster and Sasha Rakhlin


## Previous Offerings    
- [CS 6501 Reinforcement Learning (Spring 2024)](https://bahh723.github.io/rl2024sp/){:target="_blank"}
- [CS 6501 Topics in Reinforcement Learning (Fall 2022)](https://shangtongzhang.github.io/teaching/cs6501_fall_22/index){:target="_blank"} by Prof. Shangtong Zhang  




